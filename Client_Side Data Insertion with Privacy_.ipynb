{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOumH8URBK5asEM6HMnbkxu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["The Code Given Below is Used Train the Client Dataset on the Model.\n","For Federated Learning and Data Privacy:\n","\n","Client need his csv file of his dataset to train the model.\n","\n","Any one method out of the given below method is applicable.\n","*   Can be Implemented using any labelling software and direct start from step to Loading Training and Testing Dataset\n","*   In the code below the csv file is created without labelling but by extracting features using image processing and hog transform.\n","\n","The Client data privacy is maintained by, the client himself running the code locally on his machine and only using the model parameter with the noise filtering for safeguading the privacy even more.\n","\n"],"metadata":{"id":"AoKPqU5vGazL"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import os\n","import csv\n","import cv2\n","import xgboost as xgb\n","from sklearn.svm import SVC\n","from skimage.feature import hog\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from sklearn.preprocessing import StandardScaler"],"metadata":{"id":"jhNQuoRNIxjT"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E95y1CIuF30c"},"outputs":[],"source":["#Connecting to Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["### Training Dataset"],"metadata":{"id":"xHh8KB2KJPPu"}},{"cell_type":"code","source":["folder1=\"/content/drive/MyDrive/Projects/Hackathon/glioma_tumor/train\"\n","folder2=\"/content/drive/MyDrive/Projects/Hackathon/meningioma_tumor/train\"\n","folder3=\"/content/drive/MyDrive/Projects/Hackathon/no_tumor/train\"\n","folder4=\"/content/drive/MyDrive/Projects/Hackathon/pituitary/train\""],"metadata":{"id":"1OnPbvRaIs_e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Floder 1 (Glioma Tumor)\n","hog_descs = []\n","i = 0\n","for filename in os.listdir(folder1):\n","    # print(os.path.join(folder_path, filename))\n","    img = cv2.imread(os.path.join(folder1, filename))\n","    if img is not None:\n","\n","        #resize total  image size to 200 x 200\n","        resize=(200,200)\n","        img1=cv2.resize(img,resize)\n","\n","        # Grayscaling the image dataset\n","        gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n","\n","        # Apply median filter with kernel size 3x3\n","        median_img = cv2.medianBlur(gray, 3)\n","\n","        fd, hog_image = hog(median_img, orientations=6, pixels_per_cell=(4, 4),  transform_sqrt=True,cells_per_block=(1, 1), visualize=True)\n","        # Convert the descriptor array into a DataFrame format\n","        hog_descs.append(fd)\n","        df = pd.DataFrame(fd)\n","        # print(df)\n","        print(\"descriptor shape \", i, \" : \", df.shape)\n","        i = i + 1\n","\n","df = pd.DataFrame(hog_descs)\n","i = 0\n","#add row of class\n","df[\"Class\"] = i\n","\n","#Storing previously saved feature descriptor to numpy file .\n","np.save(\"/content/drive/MyDrive/Projects/Hackathon/gliomatrain.npy\", df.to_numpy())"],"metadata":{"id":"rLcCelOEIs8J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Folder 2 (Meningioma Tumor)\n","hog_descs = []\n","i = 0\n","for filename in os.listdir(folder2):\n","    # print(os.path.join(folder_path, filename))\n","    img = cv2.imread(os.path.join(folder2, filename))\n","    if img is not None:\n","        #resize total  image size to 200 x 200\n","        resize=(200,200)\n","        img1=cv2.resize(img,resize)\n","\n","        # Grayscaling the image dataset\n","        gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n","\n","        # Apply median filter with kernel size 3x3\n","        median_img = cv2.medianBlur(gray, 3)\n","\n","        fd, hog_image = hog(median_img, orientations=6, pixels_per_cell=(4, 4),  transform_sqrt=True,cells_per_block=(1, 1), visualize=True)\n","        # Convert the descriptor array into a DataFrame format\n","        hog_descs.append(fd)\n","        df = pd.DataFrame(fd)\n","        # print(df)\n","        print(\"descriptor shape \", i, \" : \", df.shape)\n","        i = i + 1\n","\n","df = pd.DataFrame(hog_descs)\n","i = 1\n","#add row class\n","df[\"Class\"] = i\n","\n","#Storing previously saved feature descriptor to numpy file .\n","np.save(\"/content/drive/MyDrive/Projects/Hackathon/meningiomatrain.npy\", df.to_numpy())"],"metadata":{"id":"sC_xNMSJIs6L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Folder 3 (No tumor)\n","hog_descs = []\n","i = 0\n","for filename in os.listdir(folder3):\n","    # print(os.path.join(folder_path, filename))\n","    img = cv2.imread(os.path.join(folder3, filename))\n","    if img is not None:\n","        #resize total  image size to 200 x 200\n","        resize=(200,200)\n","        img1=cv2.resize(img,resize)\n","\n","        # Grayscaling the image dataset\n","        gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n","\n","        # Apply median filter with kernel size 3x3\n","        median_img = cv2.medianBlur(gray, 3)\n","\n","        fd, hog_image = hog(median_img, orientations=6, pixels_per_cell=(4, 4),  transform_sqrt=True,cells_per_block=(1, 1), visualize=True)\n","        # Convert the descriptor array into a DataFrame format\n","        hog_descs.append(fd)\n","        df = pd.DataFrame(fd)\n","        # print(df)\n","        print(\"descriptor shape \", i, \" : \", df.shape)\n","        i = i + 1\n","\n","df = pd.DataFrame(hog_descs)\n","i = 2\n","#add row class\n","df[\"Class\"] = i\n","\n","#Storing previously saved feature descriptor to numpy file .\n","np.save(\"/content/drive/MyDrive/Projects/Hackathon/notumortrain.npy\", df.to_numpy())"],"metadata":{"id":"Co9GBQ6hIs3-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Folder 4 (Pituitary)\n","hog_descs = []\n","i = 0\n","for filename in os.listdir(folder4):\n","    # print(os.path.join(folder_path, filename))\n","    img = cv2.imread(os.path.join(folder4, filename))\n","    if img is not None:\n","        #resize total  image size to 200 x 200\n","        resize=(200,200)\n","        img1=cv2.resize(img,resize)\n","\n","        # Grayscaling the image dataset\n","        gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n","\n","        # Apply median filter with kernel size 3x3\n","        median_img = cv2.medianBlur(gray, 3)\n","\n","        fd, hog_image = hog(median_img, orientations=6, pixels_per_cell=(4, 4),  transform_sqrt=True,cells_per_block=(1, 1), visualize=True)\n","        # Convert the descriptor array into a DataFrame format\n","        hog_descs.append(fd)\n","        df = pd.DataFrame(fd)\n","        # print(df)\n","        print(\"descriptor shape \", i, \" : \", df.shape)\n","        i = i + 1\n","\n","df = pd.DataFrame(hog_descs)\n","i = 3\n","#add row class\n","df[\"Class\"] = i\n","\n","#Storing previously saved feature descriptor to numpy file .\n","np.save(\"/content/drive/MyDrive/Projects/Hackathon/pituitarytrain.npy\", df.to_numpy())"],"metadata":{"id":"tv0kwn5lIs1y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the Hog features of All class Glioma, Meningioma, no tumor\n","glioma_array = np.load('/content/drive/MyDrive/Projects/Hackathon/gliomatrain.npy')\n","meningioma_array = np.load('/content/drive/MyDrive/Projects/Hackathon/meningiomatrain.npy')\n","notumor_array = np.load('/content/drive/MyDrive/Projects/Hackathon/notumortrain.npy')\n","pitutary_array = np.load('/content/drive/MyDrive/Projects/Hackathon/pituitarytrain.npy')\n","\n","concatenated_array = np.concatenate((glioma_array, meningioma_array,notumor_array,pitutary_array), axis=0)\n","\n","# prints the shape of the concatenated array\n","print(concatenated_array.shape)"],"metadata":{"id":"fD280NjZIszp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = concatenated_array[:, :15000]   # selects columns up to 15000 for x\n","y = concatenated_array[:, -1]       # selects the last column for y\n","y = y.astype(np.int64)  # for 64-bit integer"],"metadata":{"id":"XW2XRwq6Isxd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(x.shape)  # prints the shape of x\n","print(y.shape)  # prints the shape of y"],"metadata":{"id":"SLOwu2u4IsvQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Applying PCA to Final Data"],"metadata":{"id":"FZdmSoY4KtJY"}},{"cell_type":"code","source":["from sklearn.decomposition import PCA\n","# Create a PCA object\n","pca = PCA()\n","\n","# Fit the PCA model to the HOG features\n","pca.fit(x)\n","\n","# Calculate the cumulative explained variance ratio\n","cumulative_var_ratio = np.cumsum(pca.explained_variance_ratio_)\n","print(pca.explained_variance_ratio_)\n","\n","# Plot the explained variance ratio against number of components\n","plt.plot(cumulative_var_ratio)\n","plt.xlabel('Number of Components')\n","plt.ylabel('Cumulative Explained Variance Ratio')\n","plt.title('cumulative variance plot.')\n","plt.show()"],"metadata":{"id":"bIrCOPm3KkSC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["B = pca.transform(x)\n","B = pd.DataFrame(B)\n","B"],"metadata":{"id":"3RG_SvS7K1SB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pca = PCA(n_components=100)\n","pca.fit(x)"],"metadata":{"id":"ZaKIe_mVK1Oq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save the model to disk\n","import joblib\n","filename = '/content/drive/MyDrive/Projects/Hackathon/PCA_modeltrain.sav'\n","pickle.dump(pca, open(filename, 'wb'))"],"metadata":{"id":"7Oiluh2GK1NC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["B = pca.transform(x)\n","B = pd.DataFrame(B)\n","B"],"metadata":{"id":"vWI3OlNGK1Ky"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Concatinate the Class ID's\n","B=pd.concat([B, pd.DataFrame(y)],axis=1)\n","B"],"metadata":{"id":"vuqH8M_-K1Ik"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Creating Final Feature File after applying PCA\n","csv_data1=B.to_csv('/content/drive/MyDrive/Projects/Hackathon/Final_HOG_Featuretrain.csv', mode='w',header=False,index=False)"],"metadata":{"id":"U8kT4ZcPK1GY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Reading recently saved file\n","train_data = pd.read_csv('/content/drive/MyDrive/Projects/Hackathon/Final_HOG_Featuretrain.csv',header=None)"],"metadata":{"id":"T2nBFsMRLZp8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Check for NaN under a single DataFrame column\n","train_data.isnull().values.any()"],"metadata":{"id":"F-Os9A_ZLZcB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = train_data.drop(columns= 100, axis=1)"],"metadata":{"id":"uUvlcQMdLMNy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Y  = train_data[100]"],"metadata":{"id":"dCh706_8LMKJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Testing Dataset"],"metadata":{"id":"b1FLgNyqLekX"}},{"cell_type":"code","source":["folder1=\"/content/drive/MyDrive/Projects/Hackathon/glioma_tumor/test\"\n","folder2=\"/content/drive/MyDrive/Projects/Hackathon/meningioma_tumor/test\"\n","folder3=\"/content/drive/MyDrive/Projects/Hackathon/no_tumor/test\"\n","folder4=\"/content/drive/MyDrive/Projects/Hackathon/pituitary/test\""],"metadata":{"id":"EshAr56VLnmj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Floder 1 (Glioma Tumor)\n","hog_descs = []\n","i = 0\n","for filename in os.listdir(folder1):\n","    # print(os.path.join(folder_path, filename))\n","    img = cv2.imread(os.path.join(folder1, filename))\n","    if img is not None:\n","\n","        #resize total  image size to 200 x 200\n","        resize=(200,200)\n","        img1=cv2.resize(img,resize)\n","\n","        # Grayscaling the image dataset\n","        gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n","\n","        # Apply median filter with kernel size 3x3\n","        median_img = cv2.medianBlur(gray, 3)\n","\n","        fd, hog_image = hog(median_img, orientations=6, pixels_per_cell=(4, 4),  transform_sqrt=True,cells_per_block=(1, 1), visualize=True)\n","        # Convert the descriptor array into a DataFrame format\n","        hog_descs.append(fd)\n","        df = pd.DataFrame(fd)\n","        # print(df)\n","        print(\"descriptor shape \", i, \" : \", df.shape)\n","        i = i + 1\n","\n","df = pd.DataFrame(hog_descs)\n","i = 0\n","#add row of class\n","df[\"Class\"] = i\n","\n","#Storing previously saved feature descriptor to numpy file .\n","np.save(\"/content/drive/MyDrive/Projects/Hackathon/gliomatest.npy\", df.to_numpy())"],"metadata":{"id":"4oUBzgk9Lnmk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Folder 2 (Meningioma Tumor)\n","hog_descs = []\n","i = 0\n","for filename in os.listdir(folder2):\n","    # print(os.path.join(folder_path, filename))\n","    img = cv2.imread(os.path.join(folder2, filename))\n","    if img is not None:\n","        #resize total  image size to 200 x 200\n","        resize=(200,200)\n","        img1=cv2.resize(img,resize)\n","\n","        # Grayscaling the image dataset\n","        gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n","\n","        # Apply median filter with kernel size 3x3\n","        median_img = cv2.medianBlur(gray, 3)\n","\n","        fd, hog_image = hog(median_img, orientations=6, pixels_per_cell=(4, 4),  transform_sqrt=True,cells_per_block=(1, 1), visualize=True)\n","        # Convert the descriptor array into a DataFrame format\n","        hog_descs.append(fd)\n","        df = pd.DataFrame(fd)\n","        # print(df)\n","        print(\"descriptor shape \", i, \" : \", df.shape)\n","        i = i + 1\n","\n","df = pd.DataFrame(hog_descs)\n","i = 1\n","#add row class\n","df[\"Class\"] = i\n","\n","#Storing previously saved feature descriptor to numpy file .\n","np.save(\"/content/drive/MyDrive/Projects/Hackathon/meningiomatest.npy\", df.to_numpy())"],"metadata":{"id":"RQmGdVKQLnmk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Folder 3 (No tumor)\n","hog_descs = []\n","i = 0\n","for filename in os.listdir(folder3):\n","    # print(os.path.join(folder_path, filename))\n","    img = cv2.imread(os.path.join(folder3, filename))\n","    if img is not None:\n","        #resize total  image size to 200 x 200\n","        resize=(200,200)\n","        img1=cv2.resize(img,resize)\n","\n","        # Grayscaling the image dataset\n","        gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n","\n","        # Apply median filter with kernel size 3x3\n","        median_img = cv2.medianBlur(gray, 3)\n","\n","        fd, hog_image = hog(median_img, orientations=6, pixels_per_cell=(4, 4),  transform_sqrt=True,cells_per_block=(1, 1), visualize=True)\n","        # Convert the descriptor array into a DataFrame format\n","        hog_descs.append(fd)\n","        df = pd.DataFrame(fd)\n","        # print(df)\n","        print(\"descriptor shape \", i, \" : \", df.shape)\n","        i = i + 1\n","\n","df = pd.DataFrame(hog_descs)\n","i = 2\n","#add row class\n","df[\"Class\"] = i\n","\n","#Storing previously saved feature descriptor to numpy file .\n","np.save(\"/content/drive/MyDrive/Projects/Hackathon/notumortest.npy\", df.to_numpy())"],"metadata":{"id":"RaWu0u49Lnml"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Folder 4 (Pituitary)\n","hog_descs = []\n","i = 0\n","for filename in os.listdir(folder4):\n","    # print(os.path.join(folder_path, filename))\n","    img = cv2.imread(os.path.join(folder4, filename))\n","    if img is not None:\n","        #resize total  image size to 200 x 200\n","        resize=(200,200)\n","        img1=cv2.resize(img,resize)\n","\n","        # Grayscaling the image dataset\n","        gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n","\n","        # Apply median filter with kernel size 3x3\n","        median_img = cv2.medianBlur(gray, 3)\n","\n","        fd, hog_image = hog(median_img, orientations=6, pixels_per_cell=(4, 4),  transform_sqrt=True,cells_per_block=(1, 1), visualize=True)\n","        # Convert the descriptor array into a DataFrame format\n","        hog_descs.append(fd)\n","        df = pd.DataFrame(fd)\n","        # print(df)\n","        print(\"descriptor shape \", i, \" : \", df.shape)\n","        i = i + 1\n","\n","df = pd.DataFrame(hog_descs)\n","i = 3\n","#add row class\n","df[\"Class\"] = i\n","\n","#Storing previously saved feature descriptor to numpy file .\n","np.save(\"/content/drive/MyDrive/Projects/Hackathon/pituitarytest.npy\", df.to_numpy())"],"metadata":{"id":"R6Ci3lhtLnml"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the Hog features of All class Glioma, Meningioma, no tumor\n","glioma_array = np.load('/content/drive/MyDrive/Projects/Hackathon/gliomatest.npy')\n","meningioma_array = np.load('/content/drive/MyDrive/Projects/Hackathon/meningiomatest.npy')\n","notumor_array = np.load('/content/drive/MyDrive/Projects/Hackathon/notumortest.npy')\n","pitutary_array = np.load('/content/drive/MyDrive/Projects/Hackathon/pituitarytest.npy')\n","\n","concatenated_array = np.concatenate((glioma_array, meningioma_array,notumor_array,pitutary_array), axis=0)\n","\n","# prints the shape of the concatenated array\n","print(concatenated_array.shape)"],"metadata":{"id":"2sXEmBaILnmm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = concatenated_array[:, :15000]   # selects columns up to 15000 for x\n","y = concatenated_array[:, -1]       # selects the last column for y\n","y = y.astype(np.int64)  # for 64-bit integer"],"metadata":{"id":"LgG2dJ67Lnmn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(x.shape)  # prints the shape of x\n","print(y.shape)  # prints the shape of y"],"metadata":{"id":"3SJZPr_ILnmn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Applying PCA to Final Data"],"metadata":{"id":"povdBfcRLnmn"}},{"cell_type":"code","source":["from sklearn.decomposition import PCA\n","# Create a PCA object\n","pca = PCA()\n","\n","# Fit the PCA model to the HOG features\n","pca.fit(x)\n","\n","# Calculate the cumulative explained variance ratio\n","cumulative_var_ratio = np.cumsum(pca.explained_variance_ratio_)\n","print(pca.explained_variance_ratio_)\n","\n","# Plot the explained variance ratio against number of components\n","plt.plot(cumulative_var_ratio)\n","plt.xlabel('Number of Components')\n","plt.ylabel('Cumulative Explained Variance Ratio')\n","plt.title('cumulative variance plot.')\n","plt.show()"],"metadata":{"id":"AEXfchLCLnmn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["B = pca.transform(x)\n","B = pd.DataFrame(B)\n","B"],"metadata":{"id":"JAMi5neQLnmo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pca = PCA(n_components=100)\n","pca.fit(x)"],"metadata":{"id":"gkX3NILqLnmo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save the model to disk\n","import joblib\n","filename = '/content/drive/MyDrive/Projects/Hackathon/PCA_modeltest.sav'\n","pickle.dump(pca, open(filename, 'wb'))"],"metadata":{"id":"8taOq_nkLnmo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["B = pca.transform(x)\n","B = pd.DataFrame(B)\n","B"],"metadata":{"id":"irSaWHEVLnmo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Concatinate the Class ID's\n","B=pd.concat([B, pd.DataFrame(y)],axis=1)\n","B"],"metadata":{"id":"Os4a0VF3Lnmo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Creating Final Feature File after applying PCA\n","csv_data1=B.to_csv('/content/drive/MyDrive/Projects/Hackathon/Final_HOG_Featuretest.csv', mode='w',header=False,index=False)"],"metadata":{"id":"vg62mt1dLnmo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Reading recently saved file\n","train_data = pd.read_csv('/content/drive/MyDrive/Projects/Hackathon/Final_HOG_Featuretest.csv',header=None)"],"metadata":{"id":"NcL6GH8iLnmo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Check for NaN under a single DataFrame column\n","train_data.isnull().values.any()"],"metadata":{"id":"BPrCkNnCLnmo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = train_data.drop(columns= 100, axis=1)"],"metadata":{"id":"WM7lmjLiLnmp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Y  = train_data[100]"],"metadata":{"id":"ApO5buGTLnmp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Loading Training and Testing Dataset"],"metadata":{"id":"tooW_qWmMIm4"}},{"cell_type":"code","source":["!pip install flwr"],"metadata":{"id":"dMZA1W_tMM3E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import flwr as fl\n","import xgboost as xgb\n","import joblib\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split\n","import random\n","\n","# Function to add noise to model parameters for differential privacy\n","def add_noise_to_parameters(params, noise_scale=0.1):\n","    noisy_params = []\n","    for param in params:\n","        # Adding Gaussian noise to each parameter\n","        noise = np.random.normal(0, noise_scale, param.shape)\n","        noisy_params.append(param + noise)\n","    return noisy_params\n","\n","# Load and merge training and testing datasets from CSV\n","def load_and_split_data(train_csv, test_csv):\n","    # Load the datasets from CSV files\n","    train_data = pd.read_csv(train_csv)\n","    test_data = pd.read_csv(test_csv)\n","\n","    # Merge the datasets (optional, depends on your use case)\n","    data = pd.concat([train_data, test_data])\n","\n","    # Assuming the last column is the target/label\n","    X = data.iloc[:, :-1]  # All columns except the last one (features)\n","    y = data.iloc[:, -1]   # The last column (target/labels)\n","\n","    # Split into training and testing sets (adjust test_size as needed)\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","    return X_train, X_test, y_train, y_test\n","\n","# Load the saved XGBoost model\n","model = joblib.load(\"/content/drive/MyDrive/Projects/Hackathon/xgb_best_model.sav\")\n","\n","# Define a Flower client for XGBoost with differential privacy\n","class XGBClientWithPrivacy(fl.client.NumPyClient):\n","    def __init__(self, X_train, X_test, y_train, y_test):\n","        self.X_train = X_train\n","        self.X_test = X_test\n","        self.y_train = y_train\n","        self.y_test = y_test\n","\n","    def get_parameters(self):\n","        # Convert model parameters to a list of numpy arrays\n","        booster = model.get_booster()\n","        return [np.array(booster.get_fscore())]\n","\n","    def set_parameters(self, parameters):\n","        # Set model parameters from the received parameters (if using a saved model)\n","        # Convert the list of numpy arrays back to the booster format\n","        model.get_booster().load_model(parameters)\n","\n","    def fit(self, parameters, config):\n","        # Update model parameters with those received from the server\n","        self.set_parameters(parameters)\n","\n","        # Train the model locally on the client's data\n","        model.fit(self.X_train, self.y_train)\n","\n","        # Add noise to the model parameters for differential privacy\n","        updated_parameters = self.get_parameters()\n","        noisy_parameters = add_noise_to_parameters(updated_parameters, noise_scale=0.1)\n","\n","        # Return the noisy parameters to the server for aggregation\n","        return noisy_parameters, len(self.X_train), {}\n","\n","    def evaluate(self, parameters, config):\n","        # Update model parameters with those received from the server\n","        self.set_parameters(parameters)\n","\n","        # Predict using the local test set\n","        preds = model.predict(self.X_test)\n","        accuracy = accuracy_score(self.y_test, preds)\n","\n","        return accuracy, len(self.X_test), {\"accuracy\": accuracy}\n","\n","\n","# Load training and testing data (CSV files)\n","train_csv = \"/content/drive/MyDrive/Projects/Hackathon/Final_HOG_Featuretrain.csv\"\n","test_csv = \"/content/drive/MyDrive/Projects/Hackathon/Final_HOG_Featuretest.csv\"\n","X_train, X_test, y_train, y_test = load_and_split_data(train_csv, test_csv)\n","\n","# Initialize Flower client with privacy and local data\n","client = XGBClientWithPrivacy(X_train, X_test, y_train, y_test)\n","\n","# Start the Flower client and connect to the server\n","fl.client.start_numpy_client(server_address=\"0.0.0.0:8080\", client=client)\n"],"metadata":{"id":"bY3f9_jiLMFu"},"execution_count":null,"outputs":[]}]}